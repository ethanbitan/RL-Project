{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"AAPL\", \"AMZN\", \"GOOGL\", \"MSFT\", \"NVDA\", \"TSLA\"]\n",
    "\n",
    "data = {i: {t: float(row[t]) for t in tickers} \\\n",
    "    for i, row in enumerate(csv.DictReader( \\\n",
    "    open(\"nasdaq_stock_prices.csv\", mode='r'), delimiter=','))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gym.Env):\n",
    "    def __init__(self, data: dict, window_size: int, initial_balance: float, verbose: bool = False):\n",
    "        self.current_step = 0\n",
    "        self.history_prices = data\n",
    "        self.current_prices = self.history_prices[self.current_step]\n",
    "        self.max_steps = len(self.history_prices) - 1\n",
    "        self.tickers = list(self.history_prices[0].keys())\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.initial_balance = initial_balance\n",
    "        self.history_balance = {0: self.initial_balance}\n",
    "        self.current_balance = self.history_balance[self.current_step]\n",
    "\n",
    "        self.initial_shares = {t: 0 for t in self.tickers}\n",
    "        self.history_shares = {0: self.initial_shares}\n",
    "        self.current_shares = self.history_shares[self.current_step]\n",
    "\n",
    "        self.initial_value = self.initial_balance\n",
    "        self.history_value = {0: self.initial_value}\n",
    "        self.current_value = self.history_value[self.current_step]\n",
    "\n",
    "        self.action_space = spaces.Box(low = -1.0, high = 1.0, shape = (len(self.tickers),))\n",
    "        self.observation_dimension = self.window_size * (2 * len(self.tickers) + 2)  #window * (prices (n) + current_shares (n) + current_balance (1) + current_value (1) )\n",
    "        self.observation_space = spaces.Box(low = -np.inf, high = np.inf, shape = (self.observation_dimension,))\n",
    "\n",
    "        self.done = False\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_prices = self.history_prices[self.current_step]\n",
    "\n",
    "        self.history_balance = {0: self.initial_balance}\n",
    "        self.current_balance = self.initial_balance\n",
    "\n",
    "        self.history_shares = {0: self.initial_shares}\n",
    "        self.current_shares = {t: 0 for t in self.tickers}\n",
    "\n",
    "        self.initial_value = self.initial_balance\n",
    "        self.history_value = {0: self.initial_value}\n",
    "        self.current_value = self.initial_value\n",
    "        \n",
    "        self.done = False\n",
    "\n",
    "        print(f\"\\nğŸ“ˆ Step: {self.current_step}\")\n",
    "        print(f\"ğŸŸ¦ Prices: {[round(self.current_prices[t], 2) for t in self.tickers]}\")\n",
    "        print(f\"ğŸ’° Balance: {self.current_balance:.2f}\")\n",
    "        print(f\"ğŸ“Š Shares: { {t: round(self.current_shares[t], 2) for t in self.tickers} }\")\n",
    "        print(f\"ğŸ“¦ Value: {self.current_value:.2f}\")\n",
    "        return self._get_state()\n",
    "    \n",
    "    def render(self):\n",
    "        return self.history_balance, self.history_shares, self.history_value\n",
    "    \n",
    "    def _get_state(self):\n",
    "        start = max(0, self.current_step - self.window_size)\n",
    "        end = self.current_step + 1\n",
    "        prices_window = [self.history_prices[i] for i in range(start, end)]\n",
    "        balance_window = [self.history_balance[i] for i in range(start, end)]\n",
    "        shares_window = [self.history_shares[i] for i in range(start, end)]\n",
    "        value_window = [self.history_value[i] for i in range(start, end)]\n",
    "        return prices_window, balance_window, shares_window, value_window\n",
    "        \n",
    "    def step(self, action: np.ndarray):\n",
    "        if self.done:\n",
    "            return self._get_state(), 0, self.done, {}\n",
    "\n",
    "        if np.sum(action) > 1.0:\n",
    "            raise ValueError(f\"Invalid action: total buy fraction = {np.sum(action):.2f} > 1.0\")\n",
    "        \n",
    "        if any([a < -1.0 for a in action]):\n",
    "            raise ValueError(f\"Invalid action: sell fraction < -1.0\")\n",
    "\n",
    "        for i, ticker in enumerate(self.tickers):\n",
    "            act = action[i]\n",
    "            if act < 0:\n",
    "                shares_to_sell = self.current_shares[ticker] * (-act)\n",
    "                proceeds = shares_to_sell * self.current_prices[ticker]\n",
    "                self.current_balance += proceeds\n",
    "                self.current_shares[ticker] -= shares_to_sell\n",
    "\n",
    "            elif act > 0:\n",
    "                amount_to_invest = self.current_balance * act\n",
    "                shares_to_buy = amount_to_invest / self.current_prices[ticker]\n",
    "                cost = shares_to_buy * self.current_prices[ticker]\n",
    "                self.current_balance -= cost\n",
    "                self.current_shares[ticker] += shares_to_buy\n",
    "\n",
    "        previous_value = self.history_value[self.current_step]\n",
    "        self.current_value = self.current_balance + sum(self.current_shares[t] * self.current_prices[t] for t in self.tickers)\n",
    "        reward = self.current_value - previous_value\n",
    "\n",
    "        self.current_step += 1\n",
    "        self.done = self.current_step >= self.max_steps\n",
    "\n",
    "        self.current_prices = self.history_prices[self.current_step]\n",
    "        self.history_balance[self.current_step] = self.current_balance\n",
    "        self.history_shares[self.current_step] = self.current_shares.copy()\n",
    "        self.history_value[self.current_step] = self.current_value\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nğŸ“ˆ Step: {self.current_step}\")\n",
    "            print(f\"ğŸŸ¦ Prices: {[round(self.current_prices[t], 2) for t in self.tickers]}\")\n",
    "            print(f\"ğŸ’° Balance: {self.current_balance:.2f}\")\n",
    "            print(f\"ğŸ“Š Shares: { {t: round(self.current_shares[t], 2) for t in self.tickers} }\")\n",
    "            print(f\"ğŸ“¦ Value: {self.current_value:.2f}\")\n",
    "            print(f\"ğŸ”„ Reward: {reward:.2f} (Î” from {previous_value:.2f})\")\n",
    "            print(f\"ğŸ¯ Action taken: {np.round(action, 2)}\")\n",
    "\n",
    "        return self._get_state(), reward, self.done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "class BaseAgent(ABC):\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def act(self, state: np.ndarray) -> tuple[np.ndarray, int | None]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, *args):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent: BaseAgent, env: Environment, episodes: int = 1, verbose: bool = False):\n",
    "    all_rewards = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action, action_id = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if hasattr(agent, \"remember\"):\n",
    "                agent.remember(state, action_id, reward)\n",
    "            else:\n",
    "                agent.update(action_id, reward)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        if hasattr(agent, \"update\") and hasattr(agent, \"remember\"):\n",
    "            agent.update()\n",
    "\n",
    "        all_rewards.append(total_reward)\n",
    "        if verbose:\n",
    "            print(f\"ğŸ¯ Ã‰pisode {ep + 1}/{episodes} â€” Total reward: {total_reward:.2f}\")\n",
    "\n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from agents.base_agent import BaseAgent\n",
    "\n",
    "class MAB_Agent(BaseAgent):\n",
    "    def __init__(self, env: Environment, epsilon: float = 0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self.tickers = env.tickers\n",
    "        self.n_arms = len(self.tickers)\n",
    "\n",
    "        buy_actions = np.eye(self.n_arms, dtype=np.float32)\n",
    "        sell_action = -np.ones((1, self.n_arms), dtype=np.float32)\n",
    "        hold_action = np.zeros((1, self.n_arms), dtype=np.float32)\n",
    "        self.action_templates = np.vstack([buy_actions, sell_action, hold_action])\n",
    "\n",
    "        self.Q = np.zeros(len(self.action_templates))\n",
    "        self.N = np.zeros(len(self.action_templates))\n",
    "\n",
    "    def reset(self):\n",
    "        self.Q = np.zeros(len(self.action_templates))\n",
    "        self.N = np.zeros(len(self.action_templates))\n",
    "\n",
    "    def select_action_id(self) -> int:\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(len(self.action_templates))\n",
    "        else:\n",
    "            return np.argmax(self.Q)\n",
    "\n",
    "    def act(self, state=None):\n",
    "        action_id = self.select_action_id()\n",
    "        action = self.action_templates[action_id]\n",
    "        return action, action_id\n",
    "\n",
    "    def update(self, action_id, reward):\n",
    "        self.N[action_id] += 1\n",
    "        alpha = 1 / self.N[action_id]\n",
    "        self.Q[action_id] += alpha * (reward - self.Q[action_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#from agents.base_agent import BaseAgent\n",
    "\n",
    "class MC_Agent(BaseAgent):\n",
    "    def __init__(self, env: Environment, epsilon: float = 0.1, gamma: float = 1.0):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.tickers = env.tickers\n",
    "        self.n_actions = len(self.tickers) + 2  # buy one asset + sell all + hold\n",
    "        self.action_templates = np.vstack([\n",
    "            np.eye(len(self.tickers)),\n",
    "            -np.ones((1, len(self.tickers))),\n",
    "            np.zeros((1, len(self.tickers)))\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        self.Q = {}           # (state, action_id) â†’ Q-value\n",
    "        self.returns = {}     # (state, action_id) â†’ list of returns\n",
    "        self.trajectory = []  # list of (state, action_id, reward)\n",
    "\n",
    "    def _state_to_key(self, state):\n",
    "        # Convertit un Ã©tat complexe en clÃ© hashable (tuple)\n",
    "        # Ici on simplifie en flattenant tous les Ã©lÃ©ments en un seul vecteur\n",
    "        prices, balance, shares, value = state\n",
    "        flat = []\n",
    "        for d in prices + balance + shares + value:\n",
    "            if isinstance(d, dict):\n",
    "                flat.extend(list(d.values()))\n",
    "            else:\n",
    "                flat.extend(d if isinstance(d, list) else [d])\n",
    "        return tuple(np.round(flat, 2))  # On arrondit pour limiter les variations\n",
    "\n",
    "    def act(self, state):\n",
    "        state_key = self._state_to_key(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action_id = np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            q_vals = [self.Q.get((state_key, a), 0.0) for a in range(self.n_actions)]\n",
    "            action_id = np.argmax(q_vals)\n",
    "        return self.action_templates[action_id], action_id\n",
    "\n",
    "    def update(self):\n",
    "        # Monte Carlo every-visit\n",
    "        G = 0\n",
    "        visited = set()\n",
    "\n",
    "        for t in reversed(range(len(self.trajectory))):\n",
    "            state, action_id, reward = self.trajectory[t]\n",
    "            G = self.gamma * G + reward\n",
    "            key = (state, action_id)\n",
    "\n",
    "            # every-visit : on stocke G Ã  chaque passage\n",
    "            if key not in self.returns:\n",
    "                self.returns[key] = []\n",
    "            self.returns[key].append(G)\n",
    "\n",
    "            # moyenne des retours\n",
    "            self.Q[key] = np.mean(self.returns[key])\n",
    "\n",
    "        self.trajectory = []  # Clear trajectory after update\n",
    "\n",
    "    def remember(self, state, action_id, reward):\n",
    "        state_key = self._state_to_key(state)\n",
    "        self.trajectory.append((state_key, action_id, reward))\n",
    "\n",
    "    def reset(self):\n",
    "        self.trajectory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "environment = Environment(data, window_size=2, initial_balance=1000, verbose=True)\n",
    "\n",
    "mab_agent_optimal = MAB_Agent(environment, epsilon=0.5)\n",
    "mab_agent_random = MAB_Agent(environment, epsilon=1.0)\n",
    "mab_agent_greedy = MAB_Agent(environment, epsilon=0.1)\n",
    "mc_agent_optimal = MC_Agent(environment, epsilon=0.5, gamma=0.7)\n",
    "mc_agent_random = MC_Agent(environment, epsilon=1.0, gamma=0.7)\n",
    "mc_agent_greedy = MC_Agent(environment, epsilon=0.1, gamma=0.7)\n",
    "\n",
    "\n",
    "results_mab_mab_agent_optimal = train_agent(mab_agent_optimal, environment, episodes=100, verbose=True)\n",
    "results_mab_agent_random = train_agent(mab_agent_random, environment, episodes=100, verbose=True)\n",
    "results_mab_agent_greedy = train_agent(mab_agent_greedy, environment, episodes=100, verbose=True)\n",
    "results_mc_agent_optimal = train_agent(mc_agent_optimal, environment, episodes=100, verbose=True)\n",
    "results_mc_agent_random = train_agent(mc_agent_random, environment, episodes=100, verbose=True)\n",
    "results_mc_agent_greedy = train_agent(mc_agent_greedy, environment, episodes=100, verbose=True)\n",
    "\n",
    "plt.plot(results_mab_agent_greedy, label=f\"Greedy {mab_agent_greedy.epsilon}\")\n",
    "plt.plot(results_mab_agent_random, label=f\"Random {mab_agent_random.epsilon}\")\n",
    "plt.plot(results_mab_mab_agent_optimal, label=f\"Optimal {mab_agent_optimal.epsilon}\")\n",
    "plt.plot(results_mc_agent_optimal, label=f\"MC {mc_agent_optimal.epsilon}\")\n",
    "plt.plot(results_mc_agent_random, label=f\"MC {mc_agent_random.epsilon}\")\n",
    "plt.plot(results_mc_agent_greedy, label=f\"MC {mc_agent_greedy.epsilon}\")\n",
    "plt.legend()\n",
    "plt.title(\"Total rewards over episodes\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total rewards\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Greedy: \", np.mean(results_mab_agent_greedy))\n",
    "print(\"Random: \", np.mean(results_mab_agent_random))\n",
    "print(\"Optimal: \", np.mean(results_mab_mab_agent_optimal))\n",
    "print(\"MC Greedy: \", np.mean(results_mc_agent_greedy))\n",
    "print(\"MC Random: \", np.mean(results_mc_agent_random))\n",
    "print(\"MC Optimal: \", np.mean(results_mc_agent_optimal))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtenv-rl-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
